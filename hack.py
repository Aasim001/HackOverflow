# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YEG_jdlWdX0s4ezms3FHWK4N_QleHRl3
"""

! pip install kaggle

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d kazanova/sentiment140

from zipfile import ZipFile
dataset = '/content/sentiment140.zip'

with ZipFile(dataset,'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

print(stopwords.words('english'))

#data processing
#load the data from csv
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',encoding = 'ISO-8859-1')

twitter_data.shape

twitter_data.head()

column_names = ['target','ids','date','flag','user','text']
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',names = column_names, encoding = 'ISO-8859-1')

twitter_data.shape

twitter_data.head()

twitter_data.isnull().sum()

twitter_data['target'].value_counts()

twitter_data.replace({'target':{4:1}},inplace = True)

twitter_data['target'].value_counts()

port_stem = PorterStemmer()

def Stemming(content):
  stemmed_content = re.sub('[^a-zA-Z]',' ',content)
  stemmed_content = stemmed_content.lower()
  stemmed_content = stemmed_content.split()
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content = ' '.join(stemmed_content)

  return stemmed_content

twitter_data['text'] = twitter_data['text'].apply(Stemming)

twitter_data.head()

print(twitter_data['text'])

print(twitter_data['target'])

x = twitter_data['text'].values
y = twitter_data['target'].values

print(x)

print(y)

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,stratify = y,random_state = 2)

print(x.shape,x_train.shape,x_test.shape)

print(x_train)

print(x_test)

vectorizer = TfidfVectorizer()

x_train= vectorizer.fit_transform(x_train)
x_test = vectorizer.transform(x_test)

print(x_train)

print(x_test)

model = LogisticRegression(max_iter=1000)

model.fit(x_train,y_train)

x_train_prediction = model.predict(x_train)
training_data_accuracy = accuracy_score(x_train_prediction,y_train)

print('Accuracy score of the training data : ', training_data_accuracy)

x_test_prediction = model.predict(x_test)
test_data_accuracy = accuracy_score(x_test_prediction,y_test)

print('Accuracy score of the test data : ', test_data_accuracy)

import pickle

filename='trained_model.sav'
pickle.dump(model,open(filename,'wb'))

loaded_model = pickle.load(open('/content/trained_model.sav','rb'))

x_new = x_test[200]
print(y_test[200])

prediction = loaded_model.predict(x_new)
print(prediction)

if (prediction[0]==0):
  print('Negative tweet')

else:
  print('Positive tweet')

x_new = x_test[1200]
print(y_test[200])

prediction = loaded_model.predict(x_new)
print(prediction)

if (prediction[0]==0):
  print('Negative tweet')

else:
  print('Positive tweet')

pip install tweepy textblob

pip install --upgrade tweepy

import tweepy
from textblob import TextBlob

class TwitterClient:
    def __init__(self):
        bearer_token = 'AAAAAAAAAAAAAAAAAAAAAFbwxAEAAAAAdH4ZsUolJczMmBCuYB0yg6FNl5E%3Dw64AoOUslzsHCXkEW9XX9uVcJvsad7oxzxNqRSWzGPrxXGEARv'
        self.client = tweepy.Client(bearer_token=bearer_token)

    def clean_tweet(self, tweet):
        """
        Clean the tweet text by removing links and special characters.
        """
        import re
        return ' '.join(re.sub(r"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \t])|(\w+://\S+)", " ", tweet).split())

    def get_tweet_sentiment(self, tweet):
        """
        Classify the sentiment of a tweet using TextBlob.
        """
        analysis = TextBlob(self.clean_tweet(tweet))
        if analysis.sentiment.polarity > 0:
            return 'positive'
        elif analysis.sentiment.polarity == 0:
            return 'neutral'
        else:
            return 'negative'

    def get_user_tweets(self, username, count=10):
        try:
            user = self.client.get_user(username=username)
            user_id = user.data.id
            tweets = self.client.get_users_tweets(id=user_id, max_results=count, tweet_fields=['text'])
            return [{'text': tweet.text, 'sentiment': self.get_tweet_sentiment(tweet.text)} for tweet in tweets.data]
        except tweepy.TweepyException as e:
            print(f"Error: {e}")
            return []

def main():
    api = TwitterClient()
    tweets = api.get_user_tweets(username='imVkohli', count=10)  # Change count to 1

    if tweets:
        ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive']
        ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative']

        print(f"Positive tweets percentage: {100 * len(ptweets) / len(tweets):.2f}%")
        print(f"Negative tweets percentage: {100 * len(ntweets) / len(tweets):.2f}%")
        print(f"Neutral tweets percentage: {100 * (len(tweets) - len(ptweets) - len(ntweets)) / len(tweets):.2f}%")

        print("\nPositive tweet:")
        for tweet in ptweets[:1]:
            print(tweet['text'])

        print("\nNegative tweet:")
        for tweet in ntweets[:1]:
            print(tweet['text'])

if __name__ == "__main__":
    main()



